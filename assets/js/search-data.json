{
  
    
        "post0": {
            "title": "Predict Visitor Purchases with a Classification Model in BQML",
            "content": "Overview . BigQuery Machine Learning (BQML, product in beta) is a new feature in BigQuery where data analysts can create, train, evaluate, and predict with machine learning models with minimal coding. . There is a newly available ecommerce dataset that has millions of Google Analytics records for the Google Merchandise Store loaded into BigQuery. In this lab you will use this data to run some typical queries that businesses would want to know about their customers’ purchasing habits. . Explore ecommerce data . Scenario: Your data analyst team exported the Google Analytics logs for an ecommerce website into BigQuery and created a new table of all the raw ecommerce visitor session data for you to explore. Using this data, you’ll try to answer a few questions. . Question: Out of the total visitors who visited our website, what % made a purchase? . #standardSQL WITH visitors AS( SELECT COUNT(DISTINCT fullVisitorId) AS total_visitors FROM `data-to-insights.ecommerce.web_analytics` ), purchasers AS( SELECT COUNT(DISTINCT fullVisitorId) AS total_purchasers FROM `data-to-insights.ecommerce.web_analytics` WHERE totals.transactions IS NOT NULL ) SELECT total_visitors, total_purchasers, total_purchasers / total_visitors AS conversion_rate FROM visitors, purchasers . The result: 2.69% . Question: What are the top 5 selling products? . SELECT p.v2ProductName, p.v2ProductCategory, SUM(p.productQuantity) AS units_sold, ROUND(SUM(p.localProductRevenue/1000000),2) AS revenue FROM `data-to-insights.ecommerce.web_analytics`, UNNEST(hits) AS h, UNNEST(h.product) AS p GROUP BY 1, 2 ORDER BY revenue DESC LIMIT 5; . The result: . Row v2ProductName v2ProductCategory units_sold revenue . 1 | Nest® Learning Thermostat 3rd Gen-USA - Stainless Steel | Nest-USA | 17651 | 870976.95 | . 2 | Nest® Cam Outdoor Security Camera - USA | Nest-USA | 16930 | 684034.55 | . 3 | Nest® Cam Indoor Security Camera - USA | Nest-USA | 14155 | 548104.47 | . 4 | Nest® Protect Smoke + CO White Wired Alarm-USA | Nest-USA | 6394 | 178937.6 | . 5 | Nest® Protect Smoke + CO White Battery Alarm-USA | Nest-USA | 6340 | 178572.4 | . Question: How many visitors bought on subsequent visits to the website? . # visitors who bought on a return visit (could have bought on first as well WITH all_visitor_stats AS ( SELECT fullvisitorid, # 741,721 unique visitors IF(COUNTIF(totals.transactions &gt; 0 AND totals.newVisits IS NULL) &gt; 0, 1, 0) AS will_buy_on_return_visit FROM `data-to-insights.ecommerce.web_analytics` GROUP BY fullvisitorid ) SELECT COUNT(DISTINCT fullvisitorid) AS total_visitors, will_buy_on_return_visit FROM all_visitor_stats GROUP BY will_buy_on_return_visit . The results: . Row total_visitors will_buy_on_return_visit . 1 | 729848 | 0 | . 2 | 11873 | 1 | . Analyzing the results, you can see that (11873 / 741721) = 1.6% of total visitors will return and purchase from the website. This includes the subset of visitors who bought on their very first session and then came back and bought again. . Question: What are some of the reasons a typical ecommerce customer will browse but not buy until a later visit? . Answer: Although there is no one right answer, one popular reason is comparison shopping between different ecommerce sites before ultimately making a purchase decision. This is very common for luxury goods where significant up-front research and comparison is required by the customer before deciding (think car purchases) but also true to a lesser extent for the merchandise on this site (t-shirts, accessories, etc). . In the world of online marketing, identifying and marketing to these future customers based on the characteristics of their first visit will increase conversion rates and reduce the outflow to competitor sites. . Identify an objective . Now you will create a Machine Learning model in BigQuery to predict whether or not a new user is likely to purchase in the future. Identifying these high-value users can help your marketing team to target them with special promotions and ad campaigns to ensure a conversion while they comparison shop between visits to your ecommerce site. . Select features and create your training dataset . Google Analytics captures a wide variety of dimensions and measures about a user’s visit on this ecommerce website. Browse the complete list of fields here and then preview the demo dataset to find useful features that will help a machine learning model understand the relationship between data about a visitor’s first time on your website and whether they will return and make a purchase. . Your team decides to test whether these two fields are good inputs for your classification model: . totals.bounces (whether the visitor left the website immediately) | totals.timeOnSite (how long the visitor was on our website) | . Question: What are the risks of only using the above two fields? . Answer: Machine learning is only as good as the training data that is fed into it. If there isn’t enough information for the model to determine and learn the relationship between your input features and your label (in this case, whether the visitor bought in the future) then you will not have an accurate model. While training a model on just these two fields is a start, you will see if they’re good enough to produce an accurate model. . SELECT * EXCEPT(fullVisitorId) FROM # features (SELECT fullVisitorId, IFNULL(totals.bounces, 0) AS bounces, IFNULL(totals.timeOnSite, 0) AS time_on_site FROM `data-to-insights.ecommerce.web_analytics` WHERE totals.newVisits = 1) JOIN (SELECT fullvisitorid, IF(COUNTIF(totals.transactions &gt; 0 AND totals.newVisits IS NULL) &gt; 0, 1, 0) AS will_buy_on_return_visit FROM `data-to-insights.ecommerce.web_analytics` GROUP BY fullvisitorid) USING (fullVisitorId) ORDER BY time_on_site DESC LIMIT 10; . Results: . Row bounces time_on_site will_buy_on_return_visit . 1 | 0 | 15047 | 0 | . 2 | 0 | 12136 | 0 | . 3 | 0 | 11201 | 0 | . 4 | 0 | 10046 | 0 | . 5 | 0 | 9974 | 0 | . 6 | 0 | 9564 | 0 | . 7 | 0 | 9520 | 0 | . 8 | 0 | 9275 | 1 | . 9 | 0 | 9138 | 0 | . 10 | 0 | 8872 | 0 | . Question: Which fields are the input features and the label? . Answer The inputs are bounces and time_on_site. The label is will_buy_on_return_visit. . Question: Which two fields are known after a visitor’s first session? . Answer: bounces and time_on_site are known after a visitor’s first session. . Question: Which field isn’t known until later in the future? . Answer: will_buy_on_return_visit is not known after the first visit. Again, you’re predicting for a subset of users who returned to your website and purchased. Since you don’t know the future at prediction time, you cannot say with certainty whether a new visitor come back and purchase. The value of building an ML model is to get the probability of future purchase based on the data gleaned about their first session. . Question: Looking at the initial data results, do you think time_on_site and bounces will be a good indicator of whether the user will return and purchase or not? . Answer: It’s often too early to tell before training and evaluating the model, but at first glance out of the top 10 time_on_site, only 1 customer returned to buy, which isn’t very promising. Let’s see how well the model does. . Select a BQML model type and specify options . Now that you have your initial features selected, you are now ready to create your first ML model in BigQuery. . There are the two model types to choose from: . Model Model Type Label Data type Example . Forecasting | linear_reg | Numeric value (typically an integer or floating point) | Forecast sales figures for next year given historical sales data. | . Classification | logistic_reg | 0 or 1 for binary classification | Classify an email as spam or not spam given the context. | . Which model type should you choose? . Since you are bucketing visitors into “will buy in future” or “won’t buy in future”, use logistic_reg in a classification model. . The following query creates a model and specifies model options. Run this query to train your model: . CREATE OR REPLACE MODEL `ecommerce.classification_model` OPTIONS ( model_type=&#39;logistic_reg&#39;, labels = [&#39;will_buy_on_return_visit&#39;] ) AS #standardSQL SELECT * EXCEPT(fullVisitorId) FROM # features (SELECT fullVisitorId, IFNULL(totals.bounces, 0) AS bounces, IFNULL(totals.timeOnSite, 0) AS time_on_site FROM `data-to-insights.ecommerce.web_analytics` WHERE totals.newVisits = 1 AND date BETWEEN &#39;20160801&#39; AND &#39;20170430&#39;) # train on first 9 months JOIN (SELECT fullvisitorid, IF(COUNTIF(totals.transactions &gt; 0 AND totals.newVisits IS NULL) &gt; 0, 1, 0) AS will_buy_on_return_visit FROM `data-to-insights.ecommerce.web_analytics` GROUP BY fullvisitorid) USING (fullVisitorId) ; . Next, you evaluate the performance of the model against new unseen evaluation data. . Evaluate classification model performance . Select your performance criteria . For classification problems in ML, you want to minimize the False Positive Rate (predict that the user will return and purchase and they don’t) and maximize the True Positive Rate (predict that the user will return and purchase and they do). . This relationship is visualized with a ROC (Receiver Operating Characteristic) curve like the one shown here, where you try to maximize the area under the curve or AUC: . . In BQML, roc_auc is simply a queryable field when evaluating your trained ML model. . Now that training is complete, run this query to evaluate how well the model performs using ML.EVALUATE: . SELECT roc_auc, CASE WHEN roc_auc &gt; .9 THEN &#39;good&#39; WHEN roc_auc &gt; .8 THEN &#39;fair&#39; WHEN roc_auc &gt; .7 THEN &#39;decent&#39; WHEN roc_auc &gt; .6 THEN &#39;not great&#39; ELSE &#39;poor&#39; END AS model_quality FROM ML.EVALUATE(MODEL ecommerce.classification_model, ( SELECT * EXCEPT(fullVisitorId) FROM # features (SELECT fullVisitorId, IFNULL(totals.bounces, 0) AS bounces, IFNULL(totals.timeOnSite, 0) AS time_on_site FROM `data-to-insights.ecommerce.web_analytics` WHERE totals.newVisits = 1 AND date BETWEEN &#39;20170501&#39; AND &#39;20170630&#39;) # eval on 2 months JOIN (SELECT fullvisitorid, IF(COUNTIF(totals.transactions &gt; 0 AND totals.newVisits IS NULL) &gt; 0, 1, 0) AS will_buy_on_return_visit FROM `data-to-insights.ecommerce.web_analytics` GROUP BY fullvisitorid) USING (fullVisitorId) )); . You should see the following result: . Row roc_auc model_quality . 1 | 0.7238561438561438 | decent | . After evaluating your model you get a roc_auc of 0.72, which shows the model has decent, but not great, predictive power. Since the goal is to get the area under the curve as close to 1.0 as possible, there is room for improvement. . Improve model performance with Feature Engineering . As was hinted at earlier, there are many more features in the dataset that may help the model better understand the relationship between a visitor’s first session and the likelihood that they will purchase on a subsequent visit. . Add some new features and create a second machine learning model called classification_model_2: . How far the visitor got in the checkout process on their first visit | Where the visitor came from (traffic source: organic search, referring site etc..) | Device category (mobile, tablet, desktop) | Geographic information (country) | . Create this second model by clicking on COMPOSE NEW QUERY: . CREATE OR REPLACE MODEL `ecommerce.classification_model_2` OPTIONS (model_type=&#39;logistic_reg&#39;, labels = [&#39;will_buy_on_return_visit&#39;]) AS WITH all_visitor_stats AS ( SELECT fullvisitorid, IF(COUNTIF(totals.transactions &gt; 0 AND totals.newVisits IS NULL) &gt; 0, 1, 0) AS will_buy_on_return_visit FROM `data-to-insights.ecommerce.web_analytics` GROUP BY fullvisitorid ) # add in new features SELECT * EXCEPT(unique_session_id) FROM ( SELECT CONCAT(fullvisitorid, CAST(visitId AS STRING)) AS unique_session_id, # labels will_buy_on_return_visit, MAX(CAST(h.eCommerceAction.action_type AS INT64)) AS latest_ecommerce_progress, # behavior on the site IFNULL(totals.bounces, 0) AS bounces, IFNULL(totals.timeOnSite, 0) AS time_on_site, IFNULL(totals.pageviews, 0) AS pageviews, # where the visitor came from trafficSource.source, trafficSource.medium, channelGrouping, # mobile or desktop device.deviceCategory, # geographic IFNULL(geoNetwork.country, &quot;&quot;) AS country FROM `data-to-insights.ecommerce.web_analytics`, UNNEST(hits) AS h JOIN all_visitor_stats USING(fullvisitorid) WHERE 1=1 # only predict for new visits AND totals.newVisits = 1 AND date BETWEEN &#39;20160801&#39; AND &#39;20170430&#39; # train 9 months GROUP BY unique_session_id, will_buy_on_return_visit, bounces, time_on_site, totals.pageviews, trafficSource.source, trafficSource.medium, channelGrouping, device.deviceCategory, country ); . A new key feature that was added to the training dataset query is the maximum checkout progress each visitor reached in their session, which is recorded in the field hits.eCommerceAction.action_type. If you search for that field in the field definitions you will see the field mapping of 6 = Completed Purchase. . The web analytics dataset has nested and repeated fields like ARRAYS which need to broken apart into separate rows in your dataset. This is accomplished by using the UNNEST() function, which you can see in the above query. . Evaluate this new model to see if there is better predictive power: . #standardSQL SELECT roc_auc, CASE WHEN roc_auc &gt; .9 THEN &#39;good&#39; WHEN roc_auc &gt; .8 THEN &#39;fair&#39; WHEN roc_auc &gt; .7 THEN &#39;decent&#39; WHEN roc_auc &gt; .6 THEN &#39;not great&#39; ELSE &#39;poor&#39; END AS model_quality FROM ML.EVALUATE(MODEL ecommerce.classification_model_2, ( WITH all_visitor_stats AS ( SELECT fullvisitorid, IF(COUNTIF(totals.transactions &gt; 0 AND totals.newVisits IS NULL) &gt; 0, 1, 0) AS will_buy_on_return_visit FROM `data-to-insights.ecommerce.web_analytics` GROUP BY fullvisitorid ) # add in new features SELECT * EXCEPT(unique_session_id) FROM ( SELECT CONCAT(fullvisitorid, CAST(visitId AS STRING)) AS unique_session_id, # labels will_buy_on_return_visit, MAX(CAST(h.eCommerceAction.action_type AS INT64)) AS latest_ecommerce_progress, # behavior on the site IFNULL(totals.bounces, 0) AS bounces, IFNULL(totals.timeOnSite, 0) AS time_on_site, totals.pageviews, # where the visitor came from trafficSource.source, trafficSource.medium, channelGrouping, # mobile or desktop device.deviceCategory, # geographic IFNULL(geoNetwork.country, &quot;&quot;) AS country FROM `data-to-insights.ecommerce.web_analytics`, UNNEST(hits) AS h JOIN all_visitor_stats USING(fullvisitorid) WHERE 1=1 # only predict for new visits AND totals.newVisits = 1 AND date BETWEEN &#39;20170501&#39; AND &#39;20170630&#39; # eval 2 months GROUP BY unique_session_id, will_buy_on_return_visit, bounces, time_on_site, totals.pageviews, trafficSource.source, trafficSource.medium, channelGrouping, device.deviceCategory, country ) )); . (Output) . Row roc_auc model_quality . 1 | 0.9094875124875125 | good | . With this new model you now get a roc_auc of 0.91 which is significantly better than the first model. . Now that you have a trained model, time to make some predictions.Predict which new visitors will come back and purchase . Next you will write a query to predict which new visitors will come back and make a purchase. . The prediction query below uses the improved classification model to predict the probability that a first-time visitor to the Google Merchandise Store will make a purchase in a later visit: . SELECT * FROM ml.PREDICT(MODEL `ecommerce.classification_model_2`, ( WITH all_visitor_stats AS ( SELECT fullvisitorid, IF(COUNTIF(totals.transactions &gt; 0 AND totals.newVisits IS NULL) &gt; 0, 1, 0) AS will_buy_on_return_visit FROM `data-to-insights.ecommerce.web_analytics` GROUP BY fullvisitorid ) SELECT CONCAT(fullvisitorid, &#39;-&#39;,CAST(visitId AS STRING)) AS unique_session_id, # labels will_buy_on_return_visit, MAX(CAST(h.eCommerceAction.action_type AS INT64)) AS latest_ecommerce_progress, # behavior on the site IFNULL(totals.bounces, 0) AS bounces, IFNULL(totals.timeOnSite, 0) AS time_on_site, totals.pageviews, # where the visitor came from trafficSource.source, trafficSource.medium, channelGrouping, # mobile or desktop device.deviceCategory, # geographic IFNULL(geoNetwork.country, &quot;&quot;) AS country FROM `data-to-insights.ecommerce.web_analytics`, UNNEST(hits) AS h JOIN all_visitor_stats USING(fullvisitorid) WHERE # only predict for new visits totals.newVisits = 1 AND date BETWEEN &#39;20170701&#39; AND &#39;20170801&#39; # test 1 month GROUP BY unique_session_id, will_buy_on_return_visit, bounces, time_on_site, totals.pageviews, trafficSource.source, trafficSource.medium, channelGrouping, device.deviceCategory, country ) ) ORDER BY predicted_will_buy_on_return_visit DESC; . The predictions are made on the last 1 month (out of 12 months) of the dataset.Your model now outputs its predictions for those July 2017 ecommerce sessions. You can see three newly added fields: . predicted_will_buy_on_return_visit: whether the model thinks the visitor will buy later (1 = yes) | predicted_will_buy_on_return_visit_probs.label: the binary classifier for yes / no | predicted_will_buy_on_return_visit.probs.prob: the confidence the model has in it’s prediction (1 = 100%) | . . Results . Of the top 6% of first-time visitors (sorted in decreasing order of predicted probability), more than 6% make a purchase in a later visit. | These users represent nearly 50% of all first-time visitors who make a purchase in a later visit. | Overall, only 0.7% of first-time visitors make a purchase in a later visit. | Targeting the top 6% of first-time increases marketing ROI by 9x vs targeting them all! | .",
            "url": "https://nb.uniqiao.com/sql/machine%20learning/2021/10/23/bqml-prediction.html",
            "relUrl": "/sql/machine%20learning/2021/10/23/bqml-prediction.html",
            "date": " • Oct 23, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Visual data analysis in banking",
            "content": "After completing this Jupyter Notebook we will be able to: . Visualize a banking dataset with MatplotLib, Seaborn and Plotly libraries. | Visually analyze single features and feature interaction. | Do a comprehensive visual data analysis for the source dataset. | . Materials and Methods . The data that we are going to use for this is a subset of an open source Bank Marketing Data Set from the UCI ML repository: https://archive.ics.uci.edu/ml/citation_policy.html. . This dataset is public available for research. The details are described in [Moro et al., 2014]. . Please include this citation if you plan to use this database:[Moro et al., 2014] S. Moro, P. Cortez and P. Rita. A Data-Driven Approach to Predict the Success of Bank Telemarketing. Decision Support Systems, Elsevier, 62:22-31, June 2014 During the work, the task of a preliminary analysis of a positive response (term deposit) to direct calls from the bank is solved. In essence, the task is the matter of bank scoring, i.e. according to the characteristics of clients (potential clients), their behavior is predicted (loan default, a wish to open a deposit, etc.). . In this lesson, we will try to give answers to a set of questions that may be relevant when analyzing banking data: . What are the most useful Python libraries for visual analysis? | How to build interactive plots? | How to visualize single features? | How to do a visual analysis for the feature interaction? | How to provide a comprehensive visual analysis for numerical and categorical features? | In addition, we will make the conclusions for the obtained results of our visual analysis to plan marketing banking campaigns more effectively. . Matplotlib is a plotting library for the Python programming language and its numerical mathematics extension NumPy. Matplotlib uses an object oriented API to embed plots in Python applications. . Seaborn is a Python visualization library based on matplotlib. It provides a high-level interface for drawing attractive statistical graphics. Seaborn provides an API on top of Matplotlib that offers sane choices for plot style and color defaults, defines simple high-level functions for common statistical plot types, and integrates with the functionality provided by Pandas DataFrames. . Plotly is an interactive, open-source plotting library that supports over 40 unique chart types covering a wide range of statistical, financial, geographic, scientific, and 3-dimensional use-cases. Built on top of the Plotly JavaScript library (plotly.js), plotly enables Python users to create beautiful interactive web-based visualizations that can be displayed in Jupyter notebooks and saved to standalone HTML files. . Import Libraries . Download data using a URL. . !wget https://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank-additional.zip . --2021-10-19 17:03:39-- https://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank-additional.zip Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252 Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 444572 (434K) [application/x-httpd-php] Saving to: ‘bank-additional.zip.6’ bank-additional.zip 100%[===================&gt;] 434.15K 1.35MB/s in 0.3s 2021-10-19 17:03:39 (1.35 MB/s) - ‘bank-additional.zip.6’ saved [444572/444572] . Alternative URL for the dataset downloading. . !wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/VDA_Banking_L2/bank-additional.zip . --2021-10-19 17:03:40-- https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/VDA_Banking_L2/bank-additional.zip Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.63.118.104 Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.63.118.104|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 444572 (434K) [application/zip] Saving to: ‘bank-additional.zip.7’ bank-additional.zip 100%[===================&gt;] 434.15K --.-KB/s in 0.01s 2021-10-19 17:03:41 (32.0 MB/s) - ‘bank-additional.zip.7’ saved [444572/444572] . Unzipping to a folder. It is a good idea to apply the -o and -q when unzipping to quiet the process and overwrite any existing folders. . !unzip -o -q bank-additional.zip . Import the libraries necessary to use in this lab. We can add some aliases to make the libraries easier to use in our code and set a default figure size for further plots. Ignore the warnings. . import pandas as pd import matplotlib.pyplot as plt import numpy as np import seaborn as sns %matplotlib inline plt.rcParams[&quot;figure.figsize&quot;] = (8, 6) import warnings warnings.filterwarnings(&#39;ignore&#39;) import plotly import plotly.graph_objs as go from plotly.offline import download_plotlyjs, init_notebook_mode, iplot, plot init_notebook_mode(connected=True) . Further specify the value of the precision parameter equal to 2 to display two decimal signs (instead of 6 as default). . pd.set_option(&quot;precision&quot;, 2) pd.options.display.float_format = &#39;{:.2f}&#39;.format . Load the Dataset . In this section you will load the source dataset. . df = pd.read_csv(&#39;bank-additional/bank-additional-full.csv&#39;, sep=&#39;;&#39;) df.head(5) . age job marital education default housing loan contact month day_of_week ... campaign pdays previous poutcome emp.var.rate cons.price.idx cons.conf.idx euribor3m nr.employed y . 0 56 | housemaid | married | basic.4y | no | no | no | telephone | may | mon | ... | 1 | 999 | 0 | nonexistent | 1.10 | 93.99 | -36.40 | 4.86 | 5191.00 | no | . 1 57 | services | married | high.school | unknown | no | no | telephone | may | mon | ... | 1 | 999 | 0 | nonexistent | 1.10 | 93.99 | -36.40 | 4.86 | 5191.00 | no | . 2 37 | services | married | high.school | no | yes | no | telephone | may | mon | ... | 1 | 999 | 0 | nonexistent | 1.10 | 93.99 | -36.40 | 4.86 | 5191.00 | no | . 3 40 | admin. | married | basic.6y | no | no | no | telephone | may | mon | ... | 1 | 999 | 0 | nonexistent | 1.10 | 93.99 | -36.40 | 4.86 | 5191.00 | no | . 4 56 | services | married | high.school | no | no | yes | telephone | may | mon | ... | 1 | 999 | 0 | nonexistent | 1.10 | 93.99 | -36.40 | 4.86 | 5191.00 | no | . 5 rows × 21 columns . The target feature shows a positive behavior of a phone call during the marketing campaign. Mark the positive outcome as 1 and negative one as 0. . Use the map function for replacement the values ​​in column by passing it as an argument dictionary in form of {old_value: new_value}. . d = {&quot;no&quot;: 0, &quot;yes&quot;: 1} df[&quot;y&quot;] = df[&quot;y&quot;].map(d) . Attribute Information . Output the column (feature) names: . df.columns . Index([&#39;age&#39;, &#39;job&#39;, &#39;marital&#39;, &#39;education&#39;, &#39;default&#39;, &#39;housing&#39;, &#39;loan&#39;, &#39;contact&#39;, &#39;month&#39;, &#39;day_of_week&#39;, &#39;duration&#39;, &#39;campaign&#39;, &#39;pdays&#39;, &#39;previous&#39;, &#39;poutcome&#39;, &#39;emp.var.rate&#39;, &#39;cons.price.idx&#39;, &#39;cons.conf.idx&#39;, &#39;euribor3m&#39;, &#39;nr.employed&#39;, &#39;y&#39;], dtype=&#39;object&#39;) . Click to see attribute information Input features (column names): 1. `age` - client age in years (numeric) 2. `job` - type of job (categorical: `admin.`, `blue-collar`, `entrepreneur`, `housemaid`, `management`, `retired`, `self-employed`, `services`, `student`, `technician`, `unemployed`, `unknown`) 3. `marital` - marital status (categorical: `divorced`, `married`, `single`, `unknown`) 4. `education` - client education (categorical: `basic.4y`, `basic.6y`, `basic.9y`, `high.school`, `illiterate`, `professional.course`, `university.degree`, `unknown`) 5. `default` - has credit in default? (categorical: `no`, `yes`, `unknown`) 6. `housing` - has housing loan? (categorical: `no`, `yes`, `unknown`) 7. `loan` - has personal loan? (categorical: `no`, `yes`, `unknown`) 8. `contact` - contact communication type (categorical: `cellular`, `telephone`) 9. `month` - last contact month of the year (categorical: `jan`, `feb`, `mar`, ..., `nov`, `dec`) 10. `day_of_week` - last contact day of the week (categorical: `mon`, `tue`, `wed`, `thu`, `fri`) 11. `duration` - last contact duration, in seconds (numeric). 12. `campaign` - number of contacts performed for this client during this campaign (numeric, includes last contact) 13. `pdays` - number of days that have passed after the client was last contacted from the previous campaign (numeric; 999 means the client has not been previously contacted) 14. `previous` - number of contacts performed for this client before this campaign (numeric) 15. `poutcome` - outcome of the previous marketing campaign (categorical: `failure`, `nonexistent`, `success`) 16. `emp.var.rate` - employment variation rate, quarterly indicator (numeric) 17. `cons.price.idx` - consumer price index, monthly indicator (numeric) 18. `cons.conf.idx` - consumer confidence index, monthly indicator (numeric) 19. `euribor3m` - euribor 3 month rate, daily indicator (numeric) 20. `nr.employed` - number of employees, quarterly indicator (numeric) Output feature (desired target): 21. `y` - has the client subscribed a term deposit? (binary: `yes`,`no`) Let&#39;s look at the dataset size. . df.shape . (41188, 21) . The dataset contains 41188 objects (rows), for each of which 21 features are set (columns), including 1 target feature (y). . Overview of Python libraries for visual data analysis . Matplotlib . Let&#39;s start our overview of Python libraries for visual data analysis with the simplest and fastest way to visualize data from Pandas DataFrame - to use the functions plot andhist. The implementation of these functions in Pandas is based on the matplotlib library. . For each feature, you can build a separate histogram with hist function: . df[&quot;age&quot;].hist() . &lt;AxesSubplot:&gt; . The histogram shows that most of our clients are between the ages of 25 and 50, which corresponds to the actively working part of the population. . We will build a graph of the average client age depending on the marital status. To begin with, we only specify the columns we need, then calc the average values ​​and for the received DataFrame call the plot function without parameters. . df[[&quot;age&quot;, &quot;marital&quot;]].groupby(&#39;marital&#39;).mean().plot() . &lt;AxesSubplot:xlabel=&#39;marital&#39;&gt; . The plot shows that the average age of unmarried clients is significantly lower than that of the other clients. . With the kind parameter you can change the plot type, for example, to a bar chart. MATPLOTLIB allows you to configure graphics very flexibly. You can change almost anything on the chart, but you will need to look up the necessary parameters in the documentation. For example, the rot parameter is responsible for the angle of tilt signatures to the x axis. . df[[&quot;age&quot;, &quot;marital&quot;]].groupby(&quot;marital&quot;).mean().plot(kind=&quot;bar&quot;, rot=45) . &lt;AxesSubplot:xlabel=&#39;marital&#39;&gt; . Seaborn . Now let&#39;s go to the seaborn library. Seaborn is a higher-level API based on the matplotlib library. Seaborn contains more adequate default graphics settings. Also there are quite complex types of visualization in the library, which would require a large amount of code in matplotlib. . We will get acquainted with the first &quot;complex&quot; type of pair plot graphics (Scatter Plot Matrix). This visualization will help us to look at one picture as at interconnection of various features. . sns.pairplot( df[[&quot;age&quot;, &quot;duration&quot;, &quot;campaign&quot;]] ) . &lt;seaborn.axisgrid.PairGrid at 0x7f2676c8ead0&gt; . This visualization allows us to identify an interesting inverse relationship between a campaign and duration, which indicates a decrease in the duration of contact with the client with an increase in their contact quantity during the campaign. . Also with the help of seaborn you can build a distribution, for example, look at the distribution of the client age. To do this, build distplot. By default, the graph shows a histogram and Kernel Density Estimation. . sns.distplot(df.age) . &lt;AxesSubplot:xlabel=&#39;age&#39;&gt; . In order to look more for the relationship between two numerical features, there is also joint_plot - this is a hybrid Scatter Plot and Histogram (there are also histograms of feature distributions). Let&#39;s look at the relationship between the number of contacts in a campaign and the last contact duration. . sns.jointplot(x=&quot;age&quot;, y=&quot;duration&quot;, data=df, kind=&quot;scatter&quot;) . &lt;seaborn.axisgrid.JointGrid at 0x7f2675fd8b90&gt; . Another useful seaborn plot type is Box Plot (&quot;Box and whisker plot&quot;). Let&#39;s compare the age of customers for the top 5 of the most common employment forms. . top_jobs = ( df.job.value_counts().sort_values(ascending=False).head(5).index.values ) sns.boxplot( y=&quot;job&quot;, x=&quot;age&quot;, data=df[df.job.isin(top_jobs)], orient=&quot;h&quot; ) . &lt;AxesSubplot:xlabel=&#39;age&#39;, ylabel=&#39;job&#39;&gt; . The plot shows that among the top-5 client categories by the type of employment, the most senior customers represent the management, and the largest number of outliers is among the categories of admin. and technician. . And one more plot type (the last of those we consider in this chapter) is a heat map. A Heat Map allows you to look at the distribution of some numerical feature in two categories. We visualize the distribution of clients on family status and the type of employment. . job_marital_y = ( df.pivot_table( index=&quot;job&quot;, columns=&quot;marital&quot;, values=&quot;y&quot;, aggfunc=sum ) ) sns.heatmap(job_marital_y, annot=True, fmt=&quot;d&quot;, linewidths=0.5) . &lt;AxesSubplot:xlabel=&#39;marital&#39;, ylabel=&#39;job&#39;&gt; . The plot shows that the largest number of attracted clients among administrative workers is married (652), and there is the smallest number of attracted clients among customers with an unknown family status. . Plotly . We looked at the visualization based on the Library Matplotlib and Seaborn. However, this is not the only option to build charts with Python. We will also get acquainted with the library plotly. Plotly is an open-source library that allows you to build interactive graphics in a jupyter notebook without having to break into JavaScript code. . The beauty of interactive graphs is that you can see the exact numerical value on mouse hover, hide the uninteresting rows in the visualization, zoom in a certain area of ​​graphics, etc. . To begin with, we build Line Plot with the distribution of the total number and the number of attracted clients by age. . age_df = ( df.groupby(&quot;age&quot;)[[&quot;y&quot;]] .sum() .join(df.groupby(&quot;age&quot;)[[&quot;y&quot;]].count(), rsuffix=&#39;_count&#39;) ) age_df.columns = [&quot;Attracted&quot;, &quot;Total Number&quot;] . In Plotly, we create the Figure object, which consists of data (list of lines that are called traces) and design/style, for which the object Layout was created. In simple cases, you can call the function iplot just for the traces list. . trace0 = go.Scatter(x=age_df.index, y=age_df[&quot;Attracted&quot;], name=&quot;Attracted&quot;) trace1 = go.Scatter(x=age_df.index, y=age_df[&quot;Total Number&quot;], name=&quot;Total Number&quot;) data = [trace0, trace1] layout = {&quot;title&quot;: &quot;Statistics by client age&quot;} fig = go.Figure(data=data, layout=layout) iplot(fig, show_link=False) . Let us also see the distribution of customers by months, designed by the number of attracted clients and on the total number of clients. To do this, build Bar Chart. . month_index = [&quot;jan&quot;, &quot;feb&quot;, &quot;mar&quot;, &quot;apr&quot;, &quot;may&quot;, &quot;jun&quot;, &quot;jul&quot;, &quot;aug&quot;, &quot;sep&quot;, &quot;oct&quot;, &quot;nov&quot;, &quot;dec&quot;] month_df = ( df.groupby(&quot;month&quot;)[[&quot;y&quot;]] .sum() .join(df.groupby(&quot;month&quot;)[[&quot;y&quot;]].count(), rsuffix=&#39;_count&#39;) ).reindex(month_index) month_df.columns = [&quot;Attracted&quot;, &quot;Total Number&quot;] . trace0 = go.Bar(x=month_df.index, y=month_df[&quot;Attracted&quot;], name=&quot;Attracted&quot;) trace1 = go.Bar(x=month_df.index, y=month_df[&quot;Total Number&quot;], name=&quot;Total Number&quot;) data = [trace0, trace1] layout = {&quot;title&quot;: &quot;Share of months&quot;} fig = go.Figure(data=data, layout=layout) iplot(fig, show_link=False) . plotly can build the Box plot. Consider the differences in the client age depending on the family status. . data = [] for status in df.marital.unique(): data.append(go.Box(y=df[df.marital == status].age, name=status)) iplot(data, show_link=False) . The plot clearly shows the distribution of clients by age, the presence of outliers for all categories of the family status, except for unknown. Moreover, the plot is interactive - hovering the mouse pointer to its elements allows you to obtain additional statistical characteristics of the series. Discover the characteristics. . Visual analysis of single features . Let us give the most commonly used plot types to analyze single features of data sets. . Numerical features . For the analysis of numerical features, a histogram and a box plot are most often used. . df[&quot;age&quot;].hist() . &lt;AxesSubplot:&gt; . Build a box plot for the cons.price.idx feature with sns.boxplot function. . sns.boxplot(df[&#39;cons.price.idx&#39;]) . &lt;AxesSubplot:xlabel=&#39;cons.price.idx&#39;&gt; . Categorical features . Use the countplot graphics for effective analysis of categorical features. It&#39;s effective to use the graphics of the type CountPlot for analyzing categorical features. . Calculate the client distribution of marital status. . df[&quot;marital&quot;].value_counts().head() . married 24928 single 11568 divorced 4612 unknown 80 Name: marital, dtype: int64 . Let&#39;s calculate the client distribution on the fact of their involvement for signing a deposit as well. . df[&quot;y&quot;].value_counts() . 0 36548 1 4640 Name: y, dtype: int64 . Present this information graphically. . sns.countplot(df[&quot;y&quot;]) . &lt;AxesSubplot:xlabel=&#39;y&#39;, ylabel=&#39;count&#39;&gt; . Build the count plot for the marital feature with sns.countplot function. . sns.countplot(df[&#39;marital&#39;]) . &lt;AxesSubplot:xlabel=&#39;marital&#39;, ylabel=&#39;count&#39;&gt; . Plot the graphical client distribution by the 5 most common types of employment. . plot = sns.countplot(df[df[&quot;job&quot;].isin(df[&quot;job&quot;].value_counts().head(5).index)][&quot;job&quot;]) plt.setp(plot.get_xticklabels(), rotation=90) . [None, None, None, None, None, None, None, None, None, None] . Visual analysis of the feature interaction . Numerical features . To analyze the interaction of numerical features, use hist (histogram), pairplot and heatmap plot functions. . We visualize the values ​​of the economy macro indicators from the dataset. . feat = [&quot;cons.price.idx&quot;, &quot;cons.conf.idx&quot;, &quot;euribor3m&quot;, &quot;nr.employed&quot;] df[feat].hist() . array([[&lt;AxesSubplot:title={&#39;center&#39;:&#39;cons.price.idx&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;cons.conf.idx&#39;}&gt;], [&lt;AxesSubplot:title={&#39;center&#39;:&#39;euribor3m&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;nr.employed&#39;}&gt;]], dtype=object) . Build a pair plot set for the feat list with sns.pairplot function. . sns.pairplot(df[feat]) . &lt;seaborn.axisgrid.PairGrid at 0x7f2673c9cd90&gt; . Build a Heat Map for the economy macro indicators correlation matrix. . sns.heatmap(df[feat].corr()) . &lt;AxesSubplot:&gt; . We see a strong interaction between the euribor3m and nr.employed features. . Numerical and categorical features . boxplot and violinplot are used for visual analysis of the numerical and categorical features. . Let&#39;s look at the age feature box plot by the target feature. . sns.boxplot(x=&quot;y&quot;, y=&quot;age&quot;, data=df) . &lt;AxesSubplot:xlabel=&#39;y&#39;, ylabel=&#39;age&#39;&gt; . Build the box plot for the marital feature with sns.boxplot function. . sns.boxplot(x=&#39;marital&#39;, y=&#39;age&#39;, data=df) . &lt;AxesSubplot:xlabel=&#39;marital&#39;, ylabel=&#39;age&#39;&gt; . You can draw a combination of boxplot and kernel density estimate with a violinplot function. A violin plot plays a similar role as a box and whisker plot. It shows the distribution of quantitative data across several levels of one (or more) categorical variables such that those distributions can be compared. . Plot the client age distribution across the target feature. . sns.violinplot(x=&quot;y&quot;, y=&quot;age&quot;, data=df) . &lt;AxesSubplot:xlabel=&#39;y&#39;, ylabel=&#39;age&#39;&gt; . It is useful to combine grouping with a boxplot. Calculate the mean client for the grouping by the housing feature values. . df.groupby(&quot;housing&quot;)[&quot;age&quot;].mean() . housing no 40.04 unknown 39.95 yes 40.01 Name: age, dtype: float64 . Build a box plot for the age feature by the housing values with sns.boxplot function. . sns.boxplot(x=&#39;housing&#39;, y=&#39;age&#39;, data=df) . &lt;AxesSubplot:xlabel=&#39;housing&#39;, ylabel=&#39;age&#39;&gt; . Categorical features . Use countplot for a visual interaction analysis between categorical features. . Calculate and visualize the interaction between target and client marital status features. . pd.crosstab(df[&quot;y&quot;], df[&quot;marital&quot;]) . marital divorced married single unknown . y . 0 4136 | 22396 | 9948 | 68 | . 1 476 | 2532 | 1620 | 12 | . sns.countplot(x=&quot;marital&quot;, hue=&quot;y&quot;, data=df) . &lt;AxesSubplot:xlabel=&#39;marital&#39;, ylabel=&#39;count&#39;&gt; . Build the count plot for the month feature by the y feature target values with sns.countplot function. . sns.countplot(x=&#39;month&#39;, hue=&#39;y&#39;, data=df) . &lt;AxesSubplot:xlabel=&#39;month&#39;, ylabel=&#39;count&#39;&gt; . Comprehensive visual analysis of the source banking dataset . Create the categorical and numerical lists for the correspondent dataset features. . Let&#39;s look at the distribution of numerical features with hist function. . categorical = [] numerical = [] for feature in df.columns: if df[feature].dtype == object: categorical.append(feature) else: numerical.append(feature) df[numerical].hist(figsize=(20,12), bins=100, color=&#39;lightgreen&#39;) . array([[&lt;AxesSubplot:title={&#39;center&#39;:&#39;age&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;duration&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;campaign&#39;}&gt;], [&lt;AxesSubplot:title={&#39;center&#39;:&#39;pdays&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;previous&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;emp.var.rate&#39;}&gt;], [&lt;AxesSubplot:title={&#39;center&#39;:&#39;cons.price.idx&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;cons.conf.idx&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;euribor3m&#39;}&gt;], [&lt;AxesSubplot:title={&#39;center&#39;:&#39;nr.employed&#39;}&gt;, &lt;AxesSubplot:title={&#39;center&#39;:&#39;y&#39;}&gt;, &lt;AxesSubplot:&gt;]], dtype=object) . From the histograms, we see that for each numerical feature there is one or more dominant segments of values​, that is why we got pronounced peaks. . In addition, we see that the target feature is unbalanced. The number of positive outcomes is significantly lower than negative, which is quite natural for telephone marketing. As a result, the problem arises with the fact that many methods are sensitive to the imbalance of features. We will try to solve this problem later. . Next, let&#39;s look at the categorical features. . df.describe(include=[&#39;object&#39;]) . job marital education default housing loan contact month day_of_week poutcome . count 41188 | 41188 | 41188 | 41188 | 41188 | 41188 | 41188 | 41188 | 41188 | 41188 | . unique 12 | 4 | 8 | 3 | 3 | 3 | 2 | 10 | 5 | 3 | . top admin. | married | university.degree | no | yes | no | cellular | may | thu | nonexistent | . freq 10422 | 24928 | 12168 | 32588 | 21576 | 33950 | 26144 | 13769 | 8623 | 35563 | . Visualize the categorical features with bar plots. . plt.rcParams[&#39;axes.labelsize&#39;] = 20 plt.rcParams[&#39;axes.titlesize&#39;] = 20 plt.rcParams[&#39;font.size&#39;] = 20 fig, axes = plt.subplots(ncols=4, nrows=3, figsize=(24, 18)) plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=0.4) for i in range(len(categorical)): df[categorical[i]].value_counts(normalize=True).plot(kind=&#39;bar&#39;, label=categorical[i], ax=axes[i//4, i%4], color=&#39;lightgreen&#39;) axes[i//4, i%4].set_title(categorical[i]) plt.tight_layout() . As we see, for many features, some of the groups stand out, for example, in the dataset more than half of the clients are married. . Let&#39;s look at the correlation matrix (for the numerical features). . correlation_table = df.corr() correlation_table . age duration campaign pdays previous emp.var.rate cons.price.idx cons.conf.idx euribor3m nr.employed y . age 1.00 | -0.00 | 0.00 | -0.03 | 0.02 | -0.00 | 0.00 | 0.13 | 0.01 | -0.02 | 0.03 | . duration -0.00 | 1.00 | -0.07 | -0.05 | 0.02 | -0.03 | 0.01 | -0.01 | -0.03 | -0.04 | 0.41 | . campaign 0.00 | -0.07 | 1.00 | 0.05 | -0.08 | 0.15 | 0.13 | -0.01 | 0.14 | 0.14 | -0.07 | . pdays -0.03 | -0.05 | 0.05 | 1.00 | -0.59 | 0.27 | 0.08 | -0.09 | 0.30 | 0.37 | -0.32 | . previous 0.02 | 0.02 | -0.08 | -0.59 | 1.00 | -0.42 | -0.20 | -0.05 | -0.45 | -0.50 | 0.23 | . emp.var.rate -0.00 | -0.03 | 0.15 | 0.27 | -0.42 | 1.00 | 0.78 | 0.20 | 0.97 | 0.91 | -0.30 | . cons.price.idx 0.00 | 0.01 | 0.13 | 0.08 | -0.20 | 0.78 | 1.00 | 0.06 | 0.69 | 0.52 | -0.14 | . cons.conf.idx 0.13 | -0.01 | -0.01 | -0.09 | -0.05 | 0.20 | 0.06 | 1.00 | 0.28 | 0.10 | 0.05 | . euribor3m 0.01 | -0.03 | 0.14 | 0.30 | -0.45 | 0.97 | 0.69 | 0.28 | 1.00 | 0.95 | -0.31 | . nr.employed -0.02 | -0.04 | 0.14 | 0.37 | -0.50 | 0.91 | 0.52 | 0.10 | 0.95 | 1.00 | -0.35 | . y 0.03 | 0.41 | -0.07 | -0.32 | 0.23 | -0.30 | -0.14 | 0.05 | -0.31 | -0.35 | 1.00 | . We visualize the correlation matrix. . sns.heatmap(correlation_table) . &lt;AxesSubplot:&gt; . Let&#39;s look at the visualized dependences of numerical features from the target feature with scatter plots. . fig, axes = plt.subplots(ncols=4, nrows=3, figsize=(24, 18)) plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=0.4) for i in range(len(numerical)): df.plot(x=numerical[i], y=&#39;y&#39;, label=numerical[i], ax=axes[i//4, i%4], kind=&#39;scatter&#39;, color=&#39;green&#39;) axes[i//4, i%4].set_title(numerical[i]) plt.tight_layout() . As you can see, there are points that can be interpreted as outliers, however, we will not hurry to delete them because they don&#39;t seem to be true outliers. These points are too strong so we will leave them. In addition, we will use some models that are resistant to outliers. . We visualize the distribution of positive target responses by groups: . fig, axes = plt.subplots(ncols=4, nrows=3, figsize=(24, 18)) plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=0.4) for i in range(len(categorical)): df.groupby(categorical[i])[&#39;y&#39;].mean().plot(kind=&#39;bar&#39;, ax=axes[i//4, i%4], color=&#39;green&#39;) axes[i//4, i%4].set_title(categorical[i]) plt.tight_layout() . In such a form, plots are already more interesting. So we see, for many features, the chance of a positive response is significantly higher. . We also see that housing, loan and day_of_week features will hardly help us, because judging by the plots, the share of positive target responses hardly depends on them. . Conclusions . There are neither any data missing, nor explicit outliers that should be cut. But we can omit housing, loan and day_of_week features in the next steps. . The euribor3m and nr.employed features strongly correlate with emp.var.rate. Let me remind you that emp.var.rate - Employment Variation Rate is a quarterly indicator, euribor3m - euribor 3 month rate is a day indicator, and nr.employed - number of employees is a quarterly indicator. The correlation of the employment change with the number of employed issues itself is obvious, but its correlation with EURIBOR (Euro Interbank Offered Rate, the European interbank offer rate) is interesting. This indicator is based on the average interbank interest rates in Eurozone. It also has a positive effect since the higher the interest rate is, the more willingly customers will spend their money on financial tools. . Therefore, if banks want to improve their lead generation, what they should do is to improve the quality of phone conversations and run their campaigns when interest rates are high and the macroeconomic environment is stable. . Authors . Roman Yatsenko . Copyright © 2021 IBM Corporation. This notebook and its source code are released under the terms of the MIT License. .",
            "url": "https://nb.uniqiao.com/data%20visualization/machine%20learning/2021/10/19/vda-banking.html",
            "relUrl": "/data%20visualization/machine%20learning/2021/10/19/vda-banking.html",
            "date": " • Oct 19, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://nb.uniqiao.com/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://nb.uniqiao.com/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://nb.uniqiao.com/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://nb.uniqiao.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}